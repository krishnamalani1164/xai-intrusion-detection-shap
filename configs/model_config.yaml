# Model hyperparameters and training configuration

models:
  random_forest:
    n_estimators: 100
    max_depth: 10
    min_samples_split: 5
    min_samples_leaf: 2
    max_features: 'sqrt'
    bootstrap: true
    n_jobs: -1
    random_state: 42
    class_weight: 'balanced'
    
  xgboost:
    n_estimators: 100
    max_depth: 6
    learning_rate: 0.1
    subsample: 0.8
    colsample_bytree: 0.8
    reg_alpha: 0.1
    reg_lambda: 1.0
    random_state: 42
    n_jobs: -1
    eval_metric: 'logloss'
    
  neural_network:
    hidden_layers: [128, 64, 32]
    activation: 'relu'
    dropout_rate: 0.3
    learning_rate: 0.001
    batch_size: 32
    epochs: 50
    early_stopping_patience: 10
    validation_split: 0.2
    optimizer: 'adam'
    loss: 'binary_crossentropy'
    
  svm:
    kernel: 'rbf'
    C: 1.0
    gamma: 'scale'
    probability: true
    class_weight: 'balanced'
    cache_size: 200

ensemble:
  voting_classifier:
    voting: 'soft'  # 'hard' or 'soft'
    weights: [1, 1, 1, 1]  # weights for RF, XGB, NN, SVM

hyperparameter_tuning:
  method: 'grid_search'  # 'grid_search', 'random_search', 'bayesian'
  cv_folds: 5
  scoring: 'f1_macro'
  n_jobs: -1
  random_state: 42
  
  # Grid search parameters
  random_forest_grid:
    n_estimators: [50, 100, 200]
    max_depth: [5, 10, 15, null]
    min_samples_split: [2, 5, 10]
    
  xgboost_grid:
    n_estimators: [50, 100, 150]
    max_depth: [3, 6, 9]
    learning_rate: [0.01, 0.1, 0.2]

training:
  cross_validation: true
  cv_folds: 5
  shuffle: true
  random_state: 42
  
evaluation:
  metrics:
    - 'accuracy'
    - 'precision'
    - 'recall'
    - 'f1_score'
    - 'roc_auc'
    - 'confusion_matrix'
  
  threshold_optimization: true
  cost_sensitive: true

model_persistence:
  save_path: 'models/trained_models/'
  save_format: 'joblib'  # 'joblib' or 'pickle'
  save_metadata: true
  versioning: true
